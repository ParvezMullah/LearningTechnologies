{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b5f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /Users/parvez.mullah/.local/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/parvez.mullah/.local/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2a3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/lib/python3.9/site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a0f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb9242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3750396",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 3, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bq/5y4x13dd18jg6fnh438xd7fr0000gp/T/ipykernel_2385/4020613440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 3, saw 5\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv(\"test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1517ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e04aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/25 09:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f767ff20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.111.140.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcfe5a9f2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe11d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eda71f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14cf37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 09:56:53 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677810aa",
   "metadata": {},
   "source": [
    "#In session w ill cover the following\n",
    "1. PySpark\n",
    "2. Reading the dataset\n",
    "3. checking the datatypes of the column(scheme)\n",
    "4. selecting columns and indexing\n",
    "5. check describe option similar to pandas\n",
    "6. adding columns\n",
    "7. dropping columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f341f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4676919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 09:56:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFramePractice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ab1acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.111.140.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcfe5a9f2b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a484a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv(\"test1.csv\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b5fc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Experience: double (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74af6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5e1213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: double, Experience: double, Salary: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3527690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39522862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ac23e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Parvez', Age=27.0, Experience=4.0, Salary=25000.0),\n",
       " Row(Name='Atif', Age=18.0, Experience=0.0, Salary=1000.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d75e089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|  Paul|27.0|      null| 3200.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "|  null|32.0|       3.0|  400.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "|  null|45.0|      32.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f532eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  Name|Experience|\n",
      "+------+----------+\n",
      "|Parvez|       4.0|\n",
      "|  Atif|       0.0|\n",
      "|   Raj|       4.0|\n",
      "|  Paul|      null|\n",
      "|Mahesh|      40.0|\n",
      "|  null|       3.0|\n",
      "| Ammar|       8.0|\n",
      "|  null|      32.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a33e530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('Experience', 'double'),\n",
       " ('Salary', 'double')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52ae1618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 09:57:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+------------------+------------------+\n",
      "|summary| Name|             Age|        Experience|            Salary|\n",
      "+-------+-----+----------------+------------------+------------------+\n",
      "|  count|    6|               7|                 7|                 6|\n",
      "|   mean| null|            30.0|              13.0|           11250.0|\n",
      "| stddev| null|8.32666399786453|16.051998837112677|14859.576037020706|\n",
      "|    min|Ammar|            18.0|               0.0|             400.0|\n",
      "|    max|  Raj|            45.0|              40.0|           34900.0|\n",
      "+-------+-----+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76d1161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns in df\n",
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e49da425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+------------------------+\n",
      "|  Name| Age|Experience| Salary|Experience After 2 Years|\n",
      "+------+----+----------+-------+------------------------+\n",
      "|Parvez|27.0|       4.0|25000.0|                     6.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|                     2.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|                     6.0|\n",
      "|  Paul|27.0|      null| 3200.0|                    null|\n",
      "|Mahesh|null|      40.0|34900.0|                    42.0|\n",
      "|  null|32.0|       3.0|  400.0|                     5.0|\n",
      "| Ammar|34.0|       8.0|   null|                    10.0|\n",
      "|  null|45.0|      32.0|   null|                    34.0|\n",
      "+------+----+----------+-------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7107f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44213c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|  Paul|27.0|      null| 3200.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "|  null|32.0|       3.0|  400.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "|  null|45.0|      32.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680283f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c1561e1",
   "metadata": {},
   "source": [
    "Pyspark handling missing values\n",
    "1. Dropping columns\n",
    "2. Dropping rows\n",
    "3. variour parameter in dropping functionalities\n",
    "4. handling missing values by mean, median and mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a910b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9f31b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 09:57:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93dfe3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(\"test1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aed7731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|  Paul|27.0|      null| 3200.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "|  null|32.0|       3.0|  400.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "|  null|45.0|      32.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5f66634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cb869ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|  Paul|27.0|      null| 3200.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "|  null|32.0|       3.0|  400.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "|  null|45.0|      32.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "850ab829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|  Paul|27.0|      null| 3200.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "|  null|32.0|       3.0|  400.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "|  null|45.0|      32.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold\n",
    "df_pyspark.na.drop(how='any', thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a7c0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name| Age|Experience| Salary|\n",
      "+------+----+----------+-------+\n",
      "|Parvez|27.0|       4.0|25000.0|\n",
      "|  Atif|18.0|       0.0| 1000.0|\n",
      "|   Raj|27.0|       4.0| 3000.0|\n",
      "|Mahesh|null|      40.0|34900.0|\n",
      "| Ammar|34.0|       8.0|   null|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', subset=['Name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b958029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "64946f8a",
   "metadata": {},
   "source": [
    "Pyspark Dataframes\n",
    "1. Filter operation\n",
    "2. &,|,==\n",
    "3. ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a87b8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c89f2619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 09:57:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"dataframe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a8afca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"test2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a77e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Parvez|  27|         4| 25000|\n",
      "|  Atif|  18|         0|  1000|\n",
      "|   Raj|  27|         4|  3000|\n",
      "|  Paul|  27|         6|  3200|\n",
      "|Mahesh|null|        40| 34900|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38f2be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Parvez| 27|         4| 25000|\n",
      "|  Atif| 18|         0|  1000|\n",
      "|   Raj| 27|         4|  3000|\n",
      "|  Paul| 27|         6|  3200|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#salary of the people less than equan to 25000\n",
    "df_pyspark.filter(\"salary<=25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e66f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Parvez| 27|\n",
      "|  Atif| 18|\n",
      "|   Raj| 27|\n",
      "|  Paul| 27|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"salary<=25000\").select([\"Name\", \"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "484ee017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Salary|\n",
      "+------+---+------+\n",
      "|Parvez| 27| 25000|\n",
      "|  Atif| 18|  1000|\n",
      "|   Raj| 27|  3000|\n",
      "|  Paul| 27|  3200|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark[\"salary\"]<=25000).select([\"Name\", \"Age\", \"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3290ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Salary|\n",
      "+------+---+------+\n",
      "|Parvez| 27| 25000|\n",
      "|   Raj| 27|  3000|\n",
      "|  Paul| 27|  3200|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark[\"salary\"]<=25000) & (df_pyspark[\"Age\"] > 20)).select([\"Name\", \"Age\", \"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b13abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n",
      "|  Name|Age|Salary|\n",
      "+------+---+------+\n",
      "|Parvez| 27| 25000|\n",
      "|  Atif| 18|  1000|\n",
      "|   Raj| 27|  3000|\n",
      "|  Paul| 27|  3200|\n",
      "+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark[\"salary\"]<=25000) | (df_pyspark[\"Age\"] <20)).select([\"Name\", \"Age\", \"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75778a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
