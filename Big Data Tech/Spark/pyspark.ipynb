{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6899b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/parvez.mullah/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/parvez.mullah/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440dbf1",
   "metadata": {},
   "source": [
    "/usr/local/Cellar/apache-spark/3.4.1/libexec/sbin/start-all.s\n",
    "\n",
    "http://localhost:8080/\n",
    "\n",
    "This will work after you open SparkSession\n",
    "http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19cfc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    ".appName(\"SparkByExamples.com\") \\\n",
    ".config('job.local.dir', 'file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark/data') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142d2b5",
   "metadata": {},
   "source": [
    "## create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed8fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([StructField(name=\"id\", dataType=IntegerType()), \n",
    "                     StructField(name=\"name\", dataType=StringType())])\n",
    "\n",
    "data = [(1, 'Maheer'), (2, 'Wafa')]\n",
    "# df = spark.createDataFrame(data=data, schema=['id', 'name']) # take data type automatically\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)  #own schema datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e62420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Maheer|\n",
      "|  2|  Wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f776427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17306567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Maheer|\n",
      "|  2|  Wafa|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{'id': 1, 'name': 'Maheer'}, {'id': 2, 'name': 'Wafa'}]\n",
    "df = spark.createDataFrame(data=data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068123d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1378e2eb",
   "metadata": {},
   "source": [
    "## Read CSV file into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8015eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Parvez|  27|         4| 25000|\n",
      "|  Atif|  18|         0|  1000|\n",
      "|   Raj|  27|         4|  3000|\n",
      "|  Paul|  27|      null|  3200|\n",
      "|Mahesh|null|        40| 34900|\n",
      "|  null|  32|         3|   400|\n",
      "| Ammar|  34|         8|  null|\n",
      "|  null|  45|        32|  null|\n",
      "+------+----+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data/test1.csv', header=True)\n",
    "    \n",
    "df_pyspark.show()\n",
    "\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf030ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc201b20",
   "metadata": {},
   "source": [
    "Read multiple csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59ae664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Parvez|  27|         4| 25000|\n",
      "|  Atif|  18|         0|  1000|\n",
      "|   Raj|  27|         4|  3000|\n",
      "|  Paul|  27|      null|  3200|\n",
      "|Mahesh|null|        40| 34900|\n",
      "|  null|  32|         3|   400|\n",
      "| Ammar|  34|         8|  null|\n",
      "|  null|  45|        32|  null|\n",
      "|Parvez|  27|         4| 25000|\n",
      "|  Atif|  18|         0|  1000|\n",
      "|   Raj|  27|         4|  3000|\n",
      "|  Paul|  27|         6|  3200|\n",
      "|Mahesh|null|        40| 34900|\n",
      "+------+----+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(['data/test1.csv', 'data/test2.csv'], header=True)\n",
    "    \n",
    "df_pyspark.show()\n",
    "\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c693a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0172b529",
   "metadata": {},
   "source": [
    "Read all csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda8019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|PAR1\u0015\u0004\u0015Ⱥ\u0001\u0015Ⱥ\u0001L\u0015�\u000f\u0015\u0004\u0000\u0000\u0000*�l�\u0019\u0000\u0000N%\u0000\u0000^0��7\u0000\u0000N%\u0000\u0000.�\"�\u0003\u0000\u0000N%\u0000\u0000���\u0001\u0000\u0000N%\u0000\u0000N�\u0004�\u0010\u0000\u0000N%\u0000\u0000ļ�&\u0018\u0000\u0000N%\u0000\u0000��c\u0000\u001c",
      "\u0000\u0000N%\u0000\u0000�|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                    \u001f7\u0016\u0000\u0000N%\u0000\u0000��W�\\f\u0000...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000.[�?7\u0000\u0000N%...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000�\u0015J�F\u0000\u0000N%...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000@ |�\u0015\u0000\u0000N%...|\n",
      "|                                                                                    �7\u0000\u0000N%\u0000\u0000�z\u0005�F\u0000\u0000N...|\n",
      "|                                                                                    �\u001f5H\u0000\u0000N%\u0000\u0000��\\v.?...|\n",
      "|                                                                                    \u0005\u0000\u0000N%\u0000\u0000x\u0015Z�\\a\u0000\u0000N...|\n",
      "|                                                                                    &\u0000\u0000N%\u0000\u0000�I��\u0001\u0000\u0000N...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000�G�l;\u0000\u0000N%...|\n",
      "|                                                                                    ���@\u0000\u0000N%\u0000\u0000r�l$0\u0000...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000�?��\u001e",
      "\u0000\u0000N%...|\n",
      "|                                                                                    }N\u0000\u0000N%\u0000\u0000��Y\u0015\u001a\u0000\u0000N...|\n",
      "|                                                                                    �\u0010LE\u0000\u0000N%\u0000\u0000Rږ�\u001b\u0000\u0000...|\n",
      "|                                                                                    \u0018\u0000\u0000N%\u0000\u0000n�\u0018u1\u0000\u0000N...|\n",
      "|                                                                                               \u0000\u0000N%\u0000\u0000�G|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000�j�r!\u0000\u0000N%...|\n",
      "|                                                                                    H\u0000\u0000N%\u0000\u0000�>i�\u0004\u0000\u0000N...|\n",
      "|                                                                                    Jsl\u0012\u0000\u0000N%\u0000\u0000�i�\u00059\u0000...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000�\u0011��4\u0000\u0000N%...|\n",
      "|                                                                                    \u0000\u0000N%\u0000\u0000Ts�ND\u0000\u0000N%\u0000\u0000�|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PAR1\u0015\u0004\u0015Ⱥ\u0001\u0015Ⱥ\u0001L\u0015�\u000f\u0015\u0004\u0000\u0000\u0000*�l�\u0019\u0000\u0000N%\u0000\u0000^0��7\u0000\u0000N%\u0000\u0000.�\"�\u0003\u0000\u0000N%\u0000\u0000���\u0001\u0000\u0000N%\u0000\u0000N�\u0004�\u0010\u0000\u0000N%\u0000\u0000ļ�&\u0018\u0000\u0000N%\u0000\u0000��c\u0000\u001c",
      "\u0000\u0000N%\u0000\u0000�: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data/', header=True)\n",
    "    \n",
    "df_pyspark.show()\n",
    "\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d826726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+------+\n",
      "|                Name| Age|Experience|Salary|\n",
      "+--------------------+----+----------+------+\n",
      "|\u001f7\u0016\u0000\u0000N%\u0000\u0000��W�\\f\u0000...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000.[�?7\u0000\u0000N%...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000�\u0015J�F\u0000\u0000N%...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000@ |�\u0015\u0000\u0000N%...|null|      null|  null|\n",
      "|�7\u0000\u0000N%\u0000\u0000�z\u0005�F\u0000\u0000N...|null|      null|  null|\n",
      "|�\u001f5H\u0000\u0000N%\u0000\u0000��\\v.?...|null|      null|  null|\n",
      "|\u0005\u0000\u0000N%\u0000\u0000x\u0015Z�\\a\u0000\u0000N...|null|      null|  null|\n",
      "|&\u0000\u0000N%\u0000\u0000�I��\u0001\u0000\u0000N...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000�G�l;\u0000\u0000N%...|null|      null|  null|\n",
      "|���@\u0000\u0000N%\u0000\u0000r�l$0\u0000...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000�?��\u001e",
      "\u0000\u0000N%...|null|      null|  null|\n",
      "|}N\u0000\u0000N%\u0000\u0000��Y\u0015\u001a\u0000\u0000N...|null|      null|  null|\n",
      "|�\u0010LE\u0000\u0000N%\u0000\u0000Rږ�\u001b\u0000\u0000...|null|      null|  null|\n",
      "|\u0018\u0000\u0000N%\u0000\u0000n�\u0018u1\u0000\u0000N...|null|      null|  null|\n",
      "|           \u0000\u0000N%\u0000\u0000�G|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000�j�r!\u0000\u0000N%...|null|      null|  null|\n",
      "|H\u0000\u0000N%\u0000\u0000�>i�\u0004\u0000\u0000N...|null|      null|  null|\n",
      "|Jsl\u0012\u0000\u0000N%\u0000\u0000�i�\u00059\u0000...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000�\u0011��4\u0000\u0000N%...|null|      null|  null|\n",
      "|\u0000\u0000N%\u0000\u0000Ts�ND\u0000\u0000N%\u0000\u0000�|null|      null|  null|\n",
      "+--------------------+----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 11:53:44 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 4\n",
      "CSV file: file:///Users/parvez.mullah/Documents/LearningTechnologies/Big%20Data%20Tech/Spark/data/userdata1.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "schema = StructType()\n",
    "schema.add(field='Name', data_type=StringType())\n",
    "schema.add(field='Age', data_type=IntegerType())\n",
    "schema.add(field='Experience', data_type=IntegerType())\n",
    "schema.add(field='Salary', data_type=IntegerType())\n",
    "\n",
    "df_pyspark = spark.read.csv('data/',schema=schema, header=True)\n",
    "    \n",
    "df_pyspark.show()\n",
    "\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac402c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b05b7e9",
   "metadata": {},
   "source": [
    "## Write DataFrame into CSV file using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37eb7798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Maheer|\n",
      "|  2|  Wafa|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 11:53:44 ERROR FileOutputCommitter: Mkdirs failed to create file:/data/_temporary/0\n",
      "23/07/27 11:53:44 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 10)\n",
      "java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/07/27 11:53:44 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 10) (10.111.140.207 executor driver): java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "\n",
      "23/07/27 11:53:44 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n",
      "23/07/27 11:53:44 ERROR FileFormatWriter: Aborting job 9a2e5827-d308-4a28-8208-b6dd6d05b9e5.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (10.111.140.207 executor driver): java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o118.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (10.111.140.207 executor driver): java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\nCaused by: java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     11\u001b[0m display(df)\n\u001b[0;32m---> 13\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o118.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (10.111.140.207 executor driver): java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\nCaused by: java.io.IOException: Mkdirs failed to create file:/data/_temporary/0/_temporary/attempt_202307271153442475555114343851041_0010_m_000000_10 (exists=false, cwd=file:/Users/parvez.mullah/Documents/LearningTechnologies/Big Data Tech/Spark)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameWriter, dataframe\n",
    "\n",
    "data = [(1, 'Maheer'), (2, 'Wafa')]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.write.csv(path='/data', header=True, mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce202ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "329980d3",
   "metadata": {},
   "source": [
    "## Read Parquet file into Dataframe using PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfcc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/userdata1.parquet\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ecd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22580464",
   "metadata": {},
   "source": [
    "## Show in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5436b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'id': 1, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 2, 'name': 'Wasdawdeqweqweqfa'},\n",
    "       {'id': 3, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 4, 'name': 'Wasdawdeqweqweqfa'},\n",
    "       {'id': 5, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 6, 'name': 'Wasdawdeqweqweqfa'}]\n",
    "df = spark.createDataFrame(data=data)\n",
    "\n",
    "df.show(n=2)\n",
    "df.show(truncate=False, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffc419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d4c71b2",
   "metadata": {},
   "source": [
    "## withColumn() in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df015760",
   "metadata": {},
   "source": [
    "Transformation, add or remove column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111ad6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|  1|Maheaaaasasqqddwe...|\n",
      "|  2|   Wasdawdeqweqweqfa|\n",
      "|  3|Maheaaaasasqqddwe...|\n",
      "|  4|   Wasdawdeqweqweqfa|\n",
      "|  5|Maheaaaasasqqddwe...|\n",
      "|  6|   Wasdawdeqweqweqfa|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{'id': 1, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 2, 'name': 'Wasdawdeqweqweqfa'},\n",
    "       {'id': 3, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 4, 'name': 'Wasdawdeqweqweqfa'},\n",
    "       {'id': 5, 'name': 'Maheaaaasasqqddweqsaer'}, {'id': 6, 'name': 'Wasdawdeqweqweqfa'}]\n",
    "df = spark.createDataFrame(data=data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6494df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|1.0|Maheaaaasasqqddwe...|\n",
      "|2.0|   Wasdawdeqweqweqfa|\n",
      "|3.0|Maheaaaasasqqddwe...|\n",
      "|4.0|   Wasdawdeqweqweqfa|\n",
      "|5.0|Maheaaaasasqqddwe...|\n",
      "|6.0|   Wasdawdeqweqweqfa|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|101|Maheaaaasasqqddwe...|\n",
      "|102|   Wasdawdeqweqweqfa|\n",
      "|103|Maheaaaasasqqddwe...|\n",
      "|104|   Wasdawdeqweqweqfa|\n",
      "|105|Maheaaaasasqqddwe...|\n",
      "|106|   Wasdawdeqweqweqfa|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df.withColumn(colName=\"id\", col=col('id').cast('Float')).show()\n",
    "\n",
    "df.withColumn(colName=\"id\", col=col('id')+100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d3c6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                name|new name|\n",
      "+---+--------------------+--------+\n",
      "|  1|Maheaaaasasqqddwe...|New Name|\n",
      "|  2|   Wasdawdeqweqweqfa|New Name|\n",
      "|  3|Maheaaaasasqqddwe...|New Name|\n",
      "|  4|   Wasdawdeqweqweqfa|New Name|\n",
      "|  5|Maheaaaasasqqddwe...|New Name|\n",
      "|  6|   Wasdawdeqweqweqfa|New Name|\n",
      "+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new name\", lit(\"New Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680ed3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                name|            new name|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|Maheaaaasasqqddwe...|Maheaaaasasqqddwe...|\n",
      "|  2|   Wasdawdeqweqweqfa|   Wasdawdeqweqweqfa|\n",
      "|  3|Maheaaaasasqqddwe...|Maheaaaasasqqddwe...|\n",
      "|  4|   Wasdawdeqweqweqfa|   Wasdawdeqweqweqfa|\n",
      "|  5|Maheaaaasasqqddwe...|Maheaaaasasqqddwe...|\n",
      "|  6|   Wasdawdeqweqweqfa|   Wasdawdeqweqweqfa|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new name\", col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b78781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab161ac8",
   "metadata": {},
   "source": [
    "##  withColumnRenamed() usage  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24daf657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            new name|\n",
      "+---+--------------------+\n",
      "|  1|Maheaaaasasqqddwe...|\n",
      "|  2|   Wasdawdeqweqweqfa|\n",
      "|  3|Maheaaaasasqqddwe...|\n",
      "|  4|   Wasdawdeqweqweqfa|\n",
      "|  5|Maheaaaasasqqddwe...|\n",
      "|  6|   Wasdawdeqweqweqfa|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"name\", \"new name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f6b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "510b9322",
   "metadata": {},
   "source": [
    "## StructType() & StructField()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb19c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+\n",
      "| id|            name|salary|\n",
      "+---+----------------+------+\n",
      "|  1|{Maheer, Shaikh}|  4000|\n",
      "|  2|  {Wafa, Shaikh}|  3000|\n",
      "+---+----------------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "data = [(1, ('Maheer', 'Shaikh'), 4000), (2, ('Wafa', 'Shaikh'), 3000)]\n",
    "\n",
    "struct_name = StructType([\n",
    "    StructField('first_name', StringType()),\n",
    "    StructField('last_name', StringType())\n",
    "])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', dataType=IntegerType()),\n",
    "    StructField('name', dataType=struct_name),\n",
    "    StructField('salary', dataType=IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91106bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f165b984",
   "metadata": {},
   "source": [
    "## ArrayType Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbee36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|  numbers|\n",
      "+---+---------+\n",
      "|abc|[1, 2, 3]|\n",
      "|xyz|[4, 5, 6]|\n",
      "+---+---------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "data = [('abc', [1,2,3]), ('xyz', [4,5,6])]\n",
    "schema = ['id', 'nuumbers']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', StringType()),\n",
    "    StructField('numbers', ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9528992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+\n",
      "| id|  numbers|first_number|\n",
      "+---+---------+------------+\n",
      "|abc|[1, 2, 3]|         100|\n",
      "|xyz|[4, 5, 6]|         400|\n",
      "+---+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('first_number', col('numbers')[0]*100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31616af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|num1|num2|numbers|\n",
      "+----+----+-------+\n",
      "|   1|   2| [1, 2]|\n",
      "|   3|   4| [3, 4]|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "\n",
    "data = [(1,2), (3,4)]\n",
    "schema = ['num1', 'num2']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.withColumn('numbers', array(col('num1'), col('num2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a6c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a0b6a2",
   "metadata": {},
   "source": [
    "## explode(), split(), array() & array_contains() functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8307556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n",
      "| id|  name|          skills|\n",
      "+---+------+----------------+\n",
      "|  1|Maheer|[dontnet, azure]|\n",
      "|  2|  Wafa|     [java, aws]|\n",
      "+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Maheer', ['dontnet', 'azure']), (2, 'Wafa', ['java', 'aws'])]\n",
    "\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2466492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  name| skills|\n",
      "+---+------+-------+\n",
      "|  1|Maheer|dontnet|\n",
      "|  1|Maheer|  azure|\n",
      "|  2|  Wafa|   java|\n",
      "|  2|  Wafa|    aws|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df1 = df.withColumn('skills', explode(col('skills')))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cf74968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+\n",
      "| id|  name|        skills|\n",
      "+---+------+--------------+\n",
      "|  1|Maheer|dontnet, azure|\n",
      "|  2|  Wafa|      java,aws|\n",
      "+---+------+--------------+\n",
      "\n",
      "+---+------+-----------------+\n",
      "| id|  name|           skills|\n",
      "+---+------+-----------------+\n",
      "|  1|Maheer|[dontnet,  azure]|\n",
      "|  2|  Wafa|      [java, aws]|\n",
      "+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Maheer', 'dontnet, azure'), (2, 'Wafa', 'java,aws')]\n",
    "\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import explode, col, split\n",
    "\n",
    "df1 = df.withColumn('skills', split(col('skills'), pattern=','))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649d5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+---------------+\n",
      "| id|  name|           skills|has_java_skills|\n",
      "+---+------+-----------------+---------------+\n",
      "|  1|Maheer|[dontnet,  azure]|          false|\n",
      "|  2|  Wafa|      [java, aws]|           true|\n",
      "+---+------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions  import array, col, array_contains\n",
    "\n",
    "df1.withColumn('has_java_skills', array_contains(col('skills'), 'java')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88aae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c7e337",
   "metadata": {},
   "source": [
    "## MapType Column in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6453ca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, properties: map<string,string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "data = [('maheer', {\"hair\": \"black\", \"eye\": \"brawn\"}), ('waha', {\"hair\": \"black\", \"eye\": \"blue\"})]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType()),\n",
    "    StructField('properties', MapType(StringType(), StringType()))\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "display(df)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6447407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+-----+\n",
      "|name  |properties                   |hair |\n",
      "+------+-----------------------------+-----+\n",
      "|maheer|{eye -> brawn, hair -> black}|black|\n",
      "|waha  |{eye -> blue, hair -> black} |black|\n",
      "+------+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('hair', df.properties['hair']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8a90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ffd306",
   "metadata": {},
   "source": [
    "## map_keys(), map_values() & explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65a8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+----+-----+\n",
      "|name  |properties                   |key |value|\n",
      "+------+-----------------------------+----+-----+\n",
      "|maheer|{eye -> brawn, hair -> black}|eye |brawn|\n",
      "|maheer|{eye -> brawn, hair -> black}|hair|black|\n",
      "|waha  |{eye -> blue, hair -> black} |eye |blue |\n",
      "|waha  |{eye -> blue, hair -> black} |hair|black|\n",
      "+------+-----------------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'properties', explode(df.properties)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f08820a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+-----------+\n",
      "|name  |properties                   |keys       |\n",
      "+------+-----------------------------+-----------+\n",
      "|maheer|{eye -> brawn, hair -> black}|[eye, hair]|\n",
      "|waha  |{eye -> blue, hair -> black} |[eye, hair]|\n",
      "+------+-----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df.withColumn('keys', map_keys(df.properties)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c347e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+--------------+\n",
      "|name  |properties                   |keys          |\n",
      "+------+-----------------------------+--------------+\n",
      "|maheer|{eye -> brawn, hair -> black}|[brawn, black]|\n",
      "|waha  |{eye -> blue, hair -> black} |[blue, black] |\n",
      "+------+-----------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "df.withColumn('keys', map_values(df.properties)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f7de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a5c171",
   "metadata": {},
   "source": [
    "## Row() class in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06978d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    _1|                  _2|\n",
      "+------+--------------------+\n",
      "|maheer|{eye -> brawn, ha...|\n",
      "|  waha|{eye -> blue, hai...|\n",
      "+------+--------------------+\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row1 = Row(name='Maheer', salary=4000)\n",
    "row2 = Row(name='Wafa', salary=8000)\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41dc7714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|Maheer|  4000|\n",
      "|  Wafa|  8000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [row1, row2]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3282077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Parvez| 27|\n",
      "| Parry| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Person = Row('name', 'age')\n",
    "\n",
    "person1 = Person(\"Parvez\", 27)\n",
    "person2 = Person(\"Parry\", 26)\n",
    "\n",
    "spark.createDataFrame([person1, person2]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef92ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5a009c",
   "metadata": {},
   "source": [
    "## Column class in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "563eccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  name|gender|salary|\n",
      "+------+------+------+\n",
      "|Maheer|  male|  2000|\n",
      "|  wafa|    23| 23444|\n",
      "+------+------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "col1 = lit('abcd')\n",
    "type(col)\n",
    "\n",
    "data = [(\"Maheer\", 'male', 2000), (\"wafa\", 23, 23444)]\n",
    "\n",
    "schema = ['name', 'gender', 'salary']\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2ebe300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----------+\n",
      "|  name|gender|salary|    new_Col|\n",
      "+------+------+------+-----------+\n",
      "|Maheer|  male|  2000|new_col_val|\n",
      "|  wafa|    23| 23444|new_col_val|\n",
      "+------+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_Col\", lit('new_col_val')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f530d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Maheer|\n",
      "|  wafa|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Maheer|\n",
      "|  wafa|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Maheer|\n",
      "|  wafa|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name).show()\n",
    "\n",
    "df.select(df[\"name\"]).show()\n",
    "\n",
    "df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff8142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf3d1737",
   "metadata": {},
   "source": [
    "## when() & otherwise() functions in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f1aafa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  2|  abcd|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(2, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd4005b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|gender|\n",
      "+---+------+------+\n",
      "|  1|maheer|  Male|\n",
      "|  2| parry|  Male|\n",
      "|  2|  abcd|Female|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.select(df.id, df.name,\n",
    "          when(df.gender=='M', 'Male')\\\n",
    "          .when(df.gender=='F', 'Female')\\\n",
    ".otherwise('Other')\\\n",
    ".alias('gender'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4efb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e3984ee",
   "metadata": {},
   "source": [
    "## alias(), asc(), desc(), cast() & like() functions on Columns of dataframe in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37a1941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_id|  name|\n",
      "+------+------+\n",
      "|     1|maheer|\n",
      "|     2| parry|\n",
      "|     2|  abcd|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(2, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.select(df.id.alias('emp_id'), df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54fcad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ac41970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2| parry|     M| 40000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2|  abcd|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "044a0731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b4ce32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id.cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0aa20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.like('%r%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850c783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d92a151",
   "metadata": {},
   "source": [
    "## filter() & where() in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "385f2f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  2|  abcd|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(2, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2aa24d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.gender=='M').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e564cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+\n",
      "| id|name|gender|salary|\n",
      "+---+----+------+------+\n",
      "|  2|abcd|     F|  3000|\n",
      "+---+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.gender=='F').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43e5bba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  2|parry|     M| 40000|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.gender=='M') & (df.salary>2000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f454a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  2|parry|     M| 40000|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((df.gender=='M') & (df.salary>2000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e013f81",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4092aab",
   "metadata": {},
   "source": [
    "## distinct() & dropDuplicates() in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e7686fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000),\n",
    "       (1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe765fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9683d74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([\"gender\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee8136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df1095a",
   "metadata": {},
   "source": [
    "# orderBy() & sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7e1788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000),\n",
    "       (1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.sort(df.id.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a7349ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.id.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c68e158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.id.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fe08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f7a62d",
   "metadata": {},
   "source": [
    "## union() & unionAll() in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6f5585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000)\n",
    "       ]\n",
    "\n",
    "data2 = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(4, 'xyz','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema)\n",
    "df2 = spark.createDataFrame(data2, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3136ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  4|   xyz|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3de7c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  4|   xyz|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3411b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  4|   xyz|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5030027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339eee05",
   "metadata": {},
   "source": [
    "## sample() function in PySpark  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e7d1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "| 13|\n",
      "| 14|\n",
      "| 35|\n",
      "| 37|\n",
      "| 51|\n",
      "| 59|\n",
      "| 71|\n",
      "| 78|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(start=1, end=101)\n",
    "\n",
    "df.sample(fraction=0.1, seed=234).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc632f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7ac8e4c",
   "metadata": {},
   "source": [
    "## collect() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0fc7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "|  1|maheer|     M|  2000|\n",
      "|  2| parry|     M| 40000|\n",
      "|  3|  abcd|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n",
      "[Row(id=1, name='maheer', gender='M', salary=2000), Row(id=2, name='parry', gender='M', salary=40000), Row(id=3, name='abcd', gender='F', salary=3000), Row(id=1, name='maheer', gender='M', salary=2000), Row(id=2, name='parry', gender='M', salary=40000), Row(id=3, name='abcd', gender='F', salary=3000)]\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000),\n",
    "       (1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "list_rows = df.collect()\n",
    "print(list_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a93a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "490de628",
   "metadata": {},
   "source": [
    "## DataFrame.transform() function in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5139778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|upper_name|\n",
      "+---+------+------+------+----------+\n",
      "|  1|maheer|     M|  2000|    MAHEER|\n",
      "|  2| parry|     M| 40000|     PARRY|\n",
      "|  3|  abcd|     F|  3000|      ABCD|\n",
      "|  1|maheer|     M|  2000|    MAHEER|\n",
      "|  2| parry|     M| 40000|     PARRY|\n",
      "|  3|  abcd|     F|  3000|      ABCD|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "data = [(1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000),\n",
    "       (1, 'maheer', 'M', 2000), (2, 'parry', 'M', 40000),(3, 'abcd','F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.withColumn('upper_name', upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18db81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|MAHEER|     M|  4000|\n",
      "|  2| PARRY|     M| 80000|\n",
      "|  3|  ABCD|     F|  6000|\n",
      "|  1|MAHEER|     M|  4000|\n",
      "|  2| PARRY|     M| 80000|\n",
      "|  3|  ABCD|     F|  6000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_to_upper(df):\n",
    "    return df.withColumn('name', upper(df.name))\n",
    "\n",
    "def double_the_salary(df):\n",
    "    return df.withColumn('salary', df.salary*2)\n",
    "\n",
    "df.transform(convert_to_upper).transform(double_the_salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c93a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6df731",
   "metadata": {},
   "source": [
    "## UDF(user defined function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a8190ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----+\n",
      "| id|  name|gender|salary|bonus|\n",
      "+---+------+------+------+-----+\n",
      "|  1|maheer|     M|  2000|  100|\n",
      "|  2| parry|     M| 40000| 1000|\n",
      "|  2|  abcd|     F|  3000|  200|\n",
      "+---+------+------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----+---------+\n",
      "| id|  name|gender|salary|bonus|total_pay|\n",
      "+---+------+------+------+-----+---------+\n",
      "|  1|maheer|     M|  2000|  100|     2100|\n",
      "|  2| parry|     M| 40000| 1000|    41000|\n",
      "|  2|  abcd|     F|  3000|  200|     3200|\n",
      "+---+------+------+------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000, 100), (2, 'parry', 'M', 40000, 1000),(2, 'abcd','F', 3000, 200)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'bonus']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "def total_pay(s, b):\n",
    "    return s+b\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "total_payment = udf(lambda s, b : total_pay(s, b), IntegerType())\n",
    "\n",
    "new_df = df.withColumn('total_pay', total_payment(df.salary, df.bonus))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c5e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd14562",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Dataset) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "27f0ef11",
   "metadata": {},
   "source": [
    "It is the collection of the objects similar to list in python. its immutable and in memoery processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4370ebf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'maheer', 2: 'abcd'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, 'maheer'), (2, 'parry'),(2, 'abcd')]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1569173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|maheer|\n",
      "|  2| parry|\n",
      "|  2|  abcd|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF(schema=[\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f8c8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|maheer|\n",
      "|  2| parry|\n",
      "|  2|  abcd|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd, schema=[\"id\", \"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd795aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cf1d9e6",
   "metadata": {},
   "source": [
    "## map() transformation in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6bc735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'maheer', '1 maheer'), (2, 'parry', '2 parry'), (2, 'abcd', '2 abcd')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, 'maheer'), (2, 'parry'),(2, 'abcd')]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "new_rdd = rdd.map(lambda x: x + (f\"{x[0]} {x[1]}\",))\n",
    "\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8c25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b525000",
   "metadata": {},
   "source": [
    "## flatMap() transformation in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "686a50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parvez Mullah\n",
      "Parry Rocks\n"
     ]
    }
   ],
   "source": [
    "data = [\"Parvez Mullah\", \"Parry Rocks\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "for item in rdd.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81ad4265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Parvez', 'Mullah']\n",
      "['Parry', 'Rocks']\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x: x.split(' '))\n",
    "\n",
    "for item in rdd1.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73de9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parvez\n",
      "Mullah\n",
      "Parry\n",
      "Rocks\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for item in rdd2.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc4b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d779de7",
   "metadata": {},
   "source": [
    "## partitionBy function in PySpark  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e093f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000, \"HR\"), (2, 'parry', 'M', 40000, \"IT\"),(2, 'abcd','F', 3000, \"Product\")]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.write.parquet(\"data/sample_data\", partitionBy=\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bcc32df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|maheer|     M|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('data/sample_data/department=HR/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1845757f",
   "metadata": {},
   "source": [
    "## approx_count_distinct(), avg(), collect_list(), collect_set(), countDistinct(), count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98e6d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|maheer|     M|  2000|        HR|\n",
      "|  2| parry|     M| 40000|        IT|\n",
      "|  2|  abcd|     F|  3000|   Product|\n",
      "|  1|maheer|     M|  2000|        HR|\n",
      "|  2| parry|     M| 40000|        IT|\n",
      "|  2|  abcd|     F|  3000|   Product|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000, \"HR\"), (2, 'parry', 'M', 40000, \"IT\"),(2, 'abcd','F', 3000, \"Product\"),\n",
    "       (1, 'maheer', 'M', 2000, \"HR\"), (2, 'parry', 'M', 40000, \"IT\"),(2, 'abcd','F', 3000, \"Product\")]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c95bfb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            3|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|    15000.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg\n",
    "df.select(approx_count_distinct('salary')).show()\n",
    "df.select(avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3428f97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|collect_list(salary)                  |\n",
      "+--------------------------------------+\n",
      "|[2000, 40000, 3000, 2000, 40000, 3000]|\n",
      "+--------------------------------------+\n",
      "\n",
      "+-------------------+\n",
      "|collect_set(salary)|\n",
      "+-------------------+\n",
      "|[40000, 3000, 2000]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df.select(collect_list('salary')).show(truncate=False)\n",
    "df.select(collect_set('salary')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3af02702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     3|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     3|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count_distinct, countDistinct\n",
    "df.select(count_distinct('salary')).show()\n",
    "df.select(countDistinct('salary')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345641d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9053bedc",
   "metadata": {},
   "source": [
    "## row_number(), rank(), dense_rank() functions in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eae38dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|maheer|     M|  2000|        HR|\n",
      "|  2| parry|     M| 40000|        IT|\n",
      "|  3|  abcd|     F|  3000|   Product|\n",
      "|  4|maheer|     M|  4000|        HR|\n",
      "|  5| parry|     M| 60000|   Finance|\n",
      "|  6|  abcd|     F|  3000|   Product|\n",
      "|  1|maheer|     M|  2000|        HR|\n",
      "|  2| parry|     M| 40000|        IT|\n",
      "|  3|  abcd|     F|  3000|   Product|\n",
      "|  4|maheer|     M|  4000|        HR|\n",
      "|  5| parry|     M| 60000|   Finance|\n",
      "|  6|  abcd|     F|  3000|   Product|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M', 2000, \"HR\"), (2, 'parry', 'M', 40000, \"IT\"),(3, 'abcd','F', 3000, \"Product\"),\n",
    "       (4, 'maheer', 'M', 4000, \"HR\"), (5, 'parry', 'M', 60000, \"Finance\"),(6, 'abcd','F', 3000, \"Product\")]*2\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a215855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+----------+-----------+\n",
      "| id|  name|gender|salary|department|row_number|densed_rank|\n",
      "+---+------+------+------+----------+----------+-----------+\n",
      "|  5| parry|     M| 60000|   Finance|         1|          1|\n",
      "|  5| parry|     M| 60000|   Finance|         2|          1|\n",
      "|  1|maheer|     M|  2000|        HR|         1|          1|\n",
      "|  1|maheer|     M|  2000|        HR|         2|          1|\n",
      "|  4|maheer|     M|  4000|        HR|         3|          2|\n",
      "|  4|maheer|     M|  4000|        HR|         4|          2|\n",
      "|  2| parry|     M| 40000|        IT|         1|          1|\n",
      "|  2| parry|     M| 40000|        IT|         2|          1|\n",
      "|  3|  abcd|     F|  3000|   Product|         1|          1|\n",
      "|  6|  abcd|     F|  3000|   Product|         2|          1|\n",
      "|  3|  abcd|     F|  3000|   Product|         3|          1|\n",
      "|  6|  abcd|     F|  3000|   Product|         4|          1|\n",
      "+---+------+------+------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"department\").orderBy('salary')\n",
    "df.withColumn('row_number', row_number().over(window))\\\n",
    ".withColumn('densed_rank', dense_rank().over(window)) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1432fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
